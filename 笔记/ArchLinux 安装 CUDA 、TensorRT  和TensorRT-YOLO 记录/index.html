<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="//cdn.jsdelivr.net/gh/Yuanmxc/Blog@latest/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="//cdn.jsdelivr.net/gh/Yuanmxc/Blog@latest/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="陌上尘归处" href="http://yuanmxc.site/rss.xml"><link rel="alternate" type="application/atom+xml" title="陌上尘归处" href="http://yuanmxc.site/atom.xml"><link rel="alternate" type="application/json" title="陌上尘归处" href="http://yuanmxc.site/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/Yuanmxc/Blog@latest/css/app.css?v=0.2.5"><link rel="canonical" href="http://yuanmxc.site/%E7%AC%94%E8%AE%B0/ArchLinux%20%E5%AE%89%E8%A3%85%20CUDA%20%E3%80%81TensorRT%20%20%E5%92%8CTensorRT-YOLO%20%E8%AE%B0%E5%BD%95/"><title>| 陌上尘归处 = 陌上尘归处 = 人生无根蒂，飘如陌上尘</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline"></h1><div class="meta"><span class="item" title="创建时间：2024-07-20 13:11:46"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-07-20T13:11:46+08:00">2024-07-20</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>5.4k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>5 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">陌上尘归处</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://cdn.jsdelivr.net/gh/Yuanmxc/Picture@latest/BlogPictures/Background/p15.png"></li><li class="item" data-background-image="https://cdn.jsdelivr.net/gh/Yuanmxc/Picture@latest/BlogPictures/Background/p44.png"></li><li class="item" data-background-image="https://cdn.jsdelivr.net/gh/Yuanmxc/Picture@latest/BlogPictures/Background/p20.png"></li><li class="item" data-background-image="https://cdn.jsdelivr.net/gh/Yuanmxc/Picture@latest/BlogPictures/Background/p11.png"></li><li class="item" data-background-image="https://cdn.jsdelivr.net/gh/Yuanmxc/Picture@latest/BlogPictures/Background/p12.png"></li><li class="item" data-background-image="https://cdn.jsdelivr.net/gh/Yuanmxc/Picture@latest/BlogPictures/Background/p38.png"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://yuanmxc.site/%E7%AC%94%E8%AE%B0/ArchLinux%20%E5%AE%89%E8%A3%85%20CUDA%20%E3%80%81TensorRT%20%20%E5%92%8CTensorRT-YOLO%20%E8%AE%B0%E5%BD%95/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="//cdn.jsdelivr.net/gh/Yuanmxc/Blog@latest/images/avatar1.jpg"><meta itemprop="name" content="陌上尘"><meta itemprop="description" content="人生无根蒂，飘如陌上尘, 不乱于心，不困于情；不畏将来，不念过往。"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="陌上尘归处"></span><div class="body md" itemprop="articleBody"><h1 id="archlinux-安装-cuda-tensorrt-和tensorrt-yolo-记录"><a class="anchor" href="#archlinux-安装-cuda-tensorrt-和tensorrt-yolo-记录">#</a> ArchLinux 安装 CUDA 、TensorRT 和 TensorRT-YOLO 记录</h1><h2 id="安装tensorrt-和-cuda"><a class="anchor" href="#安装tensorrt-和-cuda">#</a> 安装 TensorRT 和 CUDA</h2><ol><li><p>根据 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xhdWdoMTIzMjEvVGVuc29yUlQtWU9MTw==">TensorRT-YOLO</span> Github 页面使用教程中的 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xhdWdoMTIzMjEvVGVuc29yUlQtWU9MTy9ibG9iL21haW4vZG9jcy9jbi9idWlsZF9hbmRfaW5zdGFsbC5tZA==">快速编译安装 </span>进行，编译前首先要安装 xmake CUDA 和 TensorRT。</p><p>xmake 简单，只需要：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>yay <span class="token parameter variable">-S</span> xmake</pre></td></tr></table></figure></li><li><p>安装 CUDA 和 TensorRT</p><ul><li><p>安装 CUDA ：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>yay <span class="token parameter variable">-S</span> cuda</pre></td></tr></table></figure><p>版本为最新版 cuda-12.5.1-1</p></li><li><p>在安装 TensorRT ：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>yay <span class="token parameter variable">-S</span> tensorrt</pre></td></tr></table></figure><p>遇到错误，AUR 提供的 TensorRT 目前仍是 10.1.0.27-1 版本，而 CUDA 和 TensorRT 的版本必须是一一对应的，TensorRT 使用 CUDA 。</p><p>吐槽一下，“适配” 这件事做的不是很好，摘自 Archlinux AUR 的 <span class="exturl" data-url="aHR0cHM6Ly9hdXIuYXJjaGxpbnV4Lm9yZy9wYWNrYWdlcy90ZW5zb3JydA==">tensorrt</span> 软件包下一个老哥的评论（已置顶）：</p><blockquote><p>它（指 AUR 提供的 tensorrt 10.1.0.27-1）无法针对 cuda 12.5 构建，将 cuda 降级到 12.4.1，它将起作用。长话短说：关于这一点有两个问题。首先，tensorrt 10.0.1 显然不支持 cuda 12.5，正如您在<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL05WSURJQS9UZW5zb3JSVC90cmVlL3YxMC4wLjE/dGFiPXJlYWRtZS1vdi1maWxlI29wdGlvbmFsLS0taWYtbm90LXVzaW5nLXRlbnNvcnJ0LWNvbnRhaW5lci1zcGVjaWZ5LXRoZS10ZW5zb3JydC1nYS1yZWxlYXNlLWJ1aWxkLXBhdGg=">上游文档</span>中看到的那样。其次，官方仓库中提供的当前版本的 nvidia 驱动程序不支持 cuda 12.5（cuda 12.5 是在没有支持它的驱动程序的情况下推送的），您可以在<span class="exturl" data-url="aHR0cHM6Ly9naXRsYWIuYXJjaGxpbnV4Lm9yZy9hcmNobGludXgvcGFja2FnaW5nL3BhY2thZ2VzL2N1ZGEvLS9pc3N1ZXMvNw==">此链接</span>的 Arch Linux cuda 软件包问题页面中阅读更多详细信息。</p></blockquote><p>两个解决办法：</p><ol><li><p>使用 downgrade 降级 CUDA 到 12.4.1 ：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token function">sudo</span> downgrade cuda</pre></td></tr></table></figure><p>但是下载速度异常缓慢，几十 kb 每秒（刚开始还比较快），故放弃。</p><p>我也尝试过在英伟达官网进行下载 CUDA ，首先你必须注册一个账号，其次，官网上 CUDA 只支持特定的几个 Linux 发行版，其中刚好没有 ArchLinux（那 AUR 的软件包是从何而来的？）</p></li><li><p>手动安装 TensorRT 的 10.2.0 版本:</p><p>在 NVIDIA 的 GitHub <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL05WSURJQS9UZW5zb3JSVA==">TensorRT</span> 下克隆存储库并进行安装。</p><p>原本打算在本机上进行安装，最后 cmake 时遇到报错，问 GPT 说是 gcc 版本和 CUDA 的编译器 nvcc 版本的问题，继续查。</p><p>最后发现 CUDA 12.4, 12.5 版本最大支持的 GCC 版本竟然是 13.2。查看本机的 gcc 版本为：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>gcc <span class="token parameter variable">-v</span></pre></td></tr><tr><td data-num="2"></td><td><pre>gcc 版本 <span class="token number">14.1</span>.1 <span class="token number">20240522</span> <span class="token punctuation">(</span>GCC<span class="token punctuation">)</span></pre></td></tr></table></figure><p>而其我并不是在英伟达官网找到的（没找到，理论上应该有），是 Google 之后在 <span class="exturl" data-url="aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbS9xdWVzdGlvbnMvNjYyMjQ1NC9jdWRhLWluY29tcGF0aWJsZS13aXRoLW15LWdjYy12ZXJzaW9u">一篇帖子</span> 下的评论找到的，这篇帖子发布于 2011 年，但我甚至能看到今年的评论。</p><p>评论的老哥给了一个 CUDA 版本和其支持的最高 GCC 版本的对应表格以及更多信息，首次评论在 Sep 23, 2017 at 14:23 ，最新编辑在 <span class="exturl" data-url="aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbS9wb3N0cy80NjM4MDYwMS9yZXZpc2lvbnM=">Jun 18 at 19:16</span> ，应该是相关开发人员？</p><p>因为降级 gcc 会破坏其他依赖，无法降级，本来打算按教程使用 docker 来用的，但是这位老哥下面给出了方法，简单来说就是安装旧版本的 gcc ，之后让 CUDA 的编译器 nvcc 使用旧版本的 gcc。</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token comment"># 安装适配的旧版本 gcc</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token function">sudo</span> pacmane <span class="token parameter variable">-S</span> gcc13</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token comment"># 在 CUDA 文件夹中添加符号链接：</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token builtin class-name">cd</span> /opt/cuda</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token function">sudo</span> <span class="token function">ln</span> <span class="token parameter variable">-s</span> /usr/bin/gcc-13 /usr/local/cuda/bin/gcc </pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token function">sudo</span> <span class="token function">ln</span> <span class="token parameter variable">-s</span> /usr/bin/g++-13 /usr/local/cuda/bin/g++</pre></td></tr></table></figure><p>这位老哥的评论下最新的评论在 <span class="exturl" data-url="aHR0cHM6Ly9zdGFja292ZXJmbG93LmNvbS9xdWVzdGlvbnMvNjYyMjQ1NC9jdWRhLWluY29tcGF0aWJsZS13aXRoLW15LWdjYy12ZXJzaW9uI2NvbW1lbnQxMzcyNjE2ODVfNDYzODA2MDE=">CommentedJan 22 at 12:27</span> ，方法有用。</p></li></ol></li></ul></li><li><p>在解决了各种版本问题后，我目前安装的环境如下：</p><ul><li><p>ArchLinux AUR 安装的 cuda-12.5.1-1。</p></li><li><p>在 NVIDIA 的 GitHub <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL05WSURJQS9UZW5zb3JSVC9yZWxlYXNlcw==">TensorRT Releases </span>安装的 TensorRT OSS v10.2.0 存储库。</p></li><li><p>yay 安装 cuda-12.5.1-1 时安装 cuda-cudnn-9.2.0.82-1。</p></li><li><p>指定上面安装的 gcc13 ，让 CUDA 的编译器 nvcc 使用。</p></li></ul><p>然后按照在 NVIDIA 的 GitHub <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL05WSURJQS9UZW5zb3JSVCNidWlsZA==">TensorRT Build</span> 的教程进行安装。</p><p>根据上面写的推荐版本 cuda-12.5.0 + cuDNN-8.9 ，使用 downgrade 降级安装 cuDNN-8.9 版本。</p><p>最后至此开始正式 build。</p></li><li><p>按教程安装 <span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9kb3dubG9hZHMvY29tcHV0ZS9tYWNoaW5lLWxlYXJuaW5nL3RlbnNvcnJ0LzEwLjIuMC90YXJzL1RlbnNvclJULTEwLjIuMC4xOS5MaW51eC54ODZfNjQtZ251LmN1ZGEtMTIuNS50YXIuZ3o=">TensorRT 10.2.0.19 for CUDA 12.5, Linux x86_64</span> ，解压并设置 <code>TRT_LIBPATH</code> 到～/.zshrc ，然后在 TensorRT 目录下执行下面操作：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token builtin class-name">cd</span> <span class="token variable">$TRT_OSSPATH</span></pre></td></tr><tr><td data-num="2"></td><td><pre> <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> build <span class="token operator">&amp;&amp;</span> <span class="token builtin class-name">cd</span> build</pre></td></tr><tr><td data-num="3"></td><td><pre> cmake <span class="token punctuation">..</span> <span class="token parameter variable">-DTRT_LIB_DIR</span><span class="token operator">=</span><span class="token variable">$TRT_LIBPATH</span> <span class="token parameter variable">-DTRT_OUT_DIR</span><span class="token operator">=</span><span class="token variable"><span class="token variable">`</span><span class="token builtin class-name">pwd</span><span class="token variable">`</span></span>/out</pre></td></tr><tr><td data-num="4"></td><td><pre> <span class="token function">make</span> -j<span class="token variable"><span class="token variable">$(</span>nproc<span class="token variable">)</span></span></pre></td></tr></table></figure><p>cmake 遇到部分警告，make 遇到错误。而且报错似乎是代码方面的错误，多方查阅也没有解决。放弃。</p></li><li><p>第二天，由于 NVIDIA 的 GitHub <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL05WSURJQS9UZW5zb3JSVCNidWlsZA==">TensorRT Build</span> 的教程所给示例都是 Ubuntu 的，并且 英伟达官网的 CUDA 也没有提供 Archlinux 版本。怀疑是不是因为系统的原因。今天尝试降级 cuda 发现下载速度不再缓慢，于是换成上面第二个方案。</p></li><li><p>使用 yay 安装 tensorRT 安装的是 tensorrt-10.1.0.27-1 版本，查看对应的 cuda 版本为 cuda-12.4.0 + cuDNN-8.9，两个都降级到对应版本。yay 安装。</p><p>有警告，单线程很慢，但是好得没报错，突然想起一个问题，在 NVIDIA 的 GitHub <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL05WSURJQS9UZW5zb3JSVCNidWlsZA==">TensorRT Build</span> 的教程中让安装了 <span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9kb3dubG9hZHMvY29tcHV0ZS9tYWNoaW5lLWxlYXJuaW5nL3RlbnNvcnJ0LzEwLjIuMC90YXJzL1RlbnNvclJULTEwLjIuMC4xOS5MaW51eC54ODZfNjQtZ251LmN1ZGEtMTIuNS50YXIuZ3o=">TensorRT 10.2.0.19 for CUDA 12.5, Linux x86_64</span> ，这个东西当时我在使用 yay 安装 TrnsorRT 时没有安装，也看到 <span class="exturl" data-url="aHR0cHM6Ly9pdm9uYmxvZy5jb20vcG9zdHMvYXJjaGxpbnV4LWluc3RhbGwtbnZpZGlhLWRyaXZlcnMv">这篇帖子</span> 说到这点：</p><blockquote><ol><li>因為軟體授權限制，無法直接用 yay 裝，需要手動複製 AUR 儲存庫</li></ol><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token function">sudo</span> pacman <span class="token parameter variable">-S</span> base-devel</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token function">git</span> clone https://aur.archlinux.org/tensorrt.git</pre></td></tr></table></figure><ol><li>然後到<span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9udmlkaWEtdGVuc29ycnQtZG93bmxvYWQ="> Nvidia 官網</span>註冊帳號，下載 <code>tar.gz</code> 檔 (注意版本需跟 CUDA 一致)，放到 <code>tensorrt</code> 目錄</li></ol></blockquote><p>原文就是繁体，这里说的去官网注册账号下载 <code>tar.gz</code> 檔说的就是 <span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9kb3dubG9hZHMvY29tcHV0ZS9tYWNoaW5lLWxlYXJuaW5nL3RlbnNvcnJ0LzEwLjIuMC90YXJzL1RlbnNvclJULTEwLjIuMC4xOS5MaW51eC54ODZfNjQtZ251LmN1ZGEtMTIuNS50YXIuZ3o=">TensorRT 10.2.0.19 for CUDA 12.5, Linux x86_64</span> 这个东西，叫作 TensorRT GA ，在 TensorRT 安装时需要指定 TensorRT GA 发布构建路径。</p><p>所以在我在上面安装了 TensorRT GA 并将其路径添加到～/.zshrc 之后，重新使用 yay 安装就成功了（当时没有发现错误是因为没有找到 TensorRT GA 是因为什么，它没写？还是我没仔细看，绕了一大圈），后来发现 CUDA 版本也是最新版，依然可以使用 yay 安装。（也就是说不用降级也可以，高版本 CUDA 是支持低版本 TensorRT 的）。</p><p>所以总结一下 CUDA 通过 yay 直接安装，TensorRT 在通过 yay 安装之前需要先安装 <span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9kb3dubG9hZHMvY29tcHV0ZS9tYWNoaW5lLWxlYXJuaW5nL3RlbnNvcnJ0LzEwLjIuMC90YXJzL1RlbnNvclJULTEwLjIuMC4xOS5MaW51eC54ODZfNjQtZ251LmN1ZGEtMTIuNS50YXIuZ3o=">TensorRT 10.2.0.19 for CUDA 12.5, Linux x86_64</span> 并设置环境变量指定路径。之后也可以通过 yay 安装。</p><p>回顾一下 Archlinux AUR 的 <span class="exturl" data-url="aHR0cHM6Ly9hdXIuYXJjaGxpbnV4Lm9yZy9wYWNrYWdlcy90ZW5zb3JydA==">tensorrt</span> 软件包下一个老哥的评论上说明的两个问题，第一个问题：</p><p>CUDA 12.4 适用于 TensorRT 10.1.0.27。 CUDA 12.5 适用于 TensorRT 10.2.0.19。1 我这里下载的是后者，后面 <code>tensorrt_yolo</code> 指定的路径也是 TensorRT 10.2.0.19 。</p><p>第二个问题是英伟达没有提供驱动，这个问题已经解决了，目前最新的驱动已经支持。</p><p>不管如何 CUDA 和 TensorRT 总算是装好了。</p></li></ol><h2 id="tensorrt_yolo"><a class="anchor" href="#tensorrt_yolo">#</a> <code>tensorrt_yolo</code></h2><p><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xhdWdoMTIzMjEvVGVuc29yUlQtWU9MTy9ibG9iL21haW4vZG9jcy9jbi9idWlsZF9hbmRfaW5zdGFsbC5tZA==">TensorRT-YOLO/docs/cn/build_and_install.md</span></p><h2 id="deploy-编译"><a class="anchor" href="#deploy-编译">#</a> <code>Deploy</code> 编译</h2><p>按照上面给的文档进行编译安装。</p><p>本机环境：</p><ul><li>Linux: gcc/g++	13.3.0（不知道上面降级和使用旧的 gcc 是否有影作用）</li><li>Xmake	v2.9.3+20240624</li><li>CUDA 12.5.1-1</li><li>TensorRT	10.1.0.27-1</li></ul><p>为了满足部署需求，您可以使用 Xmake 进行 <code>Deploy</code> 编译。此过程支持动态库和静态库的编译：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token function">git</span> clone https://github.com/laugh12321/TensorRT-YOLO</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token builtin class-name">cd</span> TensorRT-YOLO</pre></td></tr><tr><td data-num="3"></td><td><pre>xmake f <span class="token parameter variable">-k</span> shared <span class="token parameter variable">--tensorrt</span><span class="token operator">=</span><span class="token string">"C:/Program Files/NVIDIA GPU Computing Toolkit/TensorRT/v8.6.1.6"</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token comment"># xmake f -k static --tensorrt="C:/Program Files/NVIDIA GPU Computing Toolkit/TensorRT/v8.6.1.6"</span></pre></td></tr><tr><td data-num="5"></td><td><pre>xmake <span class="token parameter variable">-P</span> <span class="token builtin class-name">.</span> <span class="token parameter variable">-r</span></pre></td></tr></table></figure><p><code>--tensorrt=&quot;C:/Program Files/NVIDIA GPU Computing Toolkit/TensorRT/v8.6.1.6&quot;</code> 的路径替换为自己的路径，这个路径经过我的探索，最后发现实际在 <span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9kb3dubG9hZHMvY29tcHV0ZS9tYWNoaW5lLWxlYXJuaW5nL3RlbnNvcnJ0LzEwLjIuMC90YXJzL1RlbnNvclJULTEwLjIuMC4xOS5MaW51eC54ODZfNjQtZ251LmN1ZGEtMTIuNS50YXIuZ3o=">TensorRT 10.2.0.19 for CUDA 12.5, Linux x86_64</span> 下，在我这里是： <code>your_path/TensorRT-10.2.0.19/targets/x86_64-linux-gnu/</code></p><h2 id="编译安装-tensorrt_yolo"><a class="anchor" href="#编译安装-tensorrt_yolo">#</a> 编译安装 <code>tensorrt_yolo</code></h2><p>跳过 PyPI 安装 <code>tensorrt_yolo</code> 模块部分。</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> build</pre></td></tr><tr><td data-num="2"></td><td><pre>python <span class="token parameter variable">-m</span> build <span class="token parameter variable">--wheel</span></pre></td></tr><tr><td data-num="3"></td><td><pre>pip <span class="token function">install</span> dist/tensorrt_yolo/tensorrt_yolo-4.*-py3-none-any.whl</pre></td></tr></table></figure><p>上面是文档给的步骤，在 TensorRT-YOLO 下 执行，第三个路径在我这里不争取。我这里对应的文件是是： <code>TensorRT-YOLO/dist/tensorrt_yolo-4.0.0-py3-none-any.whl</code> ，因此路径写 <code>dist/tensorrt_yolo-4.0.0-py3-none-any.whl</code> 。</p><h2 id="跑模型推理示例"><a class="anchor" href="#跑模型推理示例">#</a> 跑模型推理示例</h2><p>按照 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xhdWdoMTIzMjEvVGVuc29yUlQtWU9MTy9ibG9iL21haW4vZGVtby9kZXRlY3QvUkVBRE1FLm1k">模型推理示例</span> 进行。</p><h3 id="模型导出"><a class="anchor" href="#模型导出">#</a> 模型导出</h3><p>首先，从 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3VsdHJhbHl0aWNzL2Fzc2V0cy9yZWxlYXNlcy9kb3dubG9hZC92OC4yLjAveW9sb3Y4cy5wdA==">YOLOv8s</span> 下载 YOLOv8s 模型并保存到 <code>models</code> 文件夹中。</p><p>注：上面是原文，这里的 <code>models</code> 文件夹经过我实际测试，实际上是 <code>/TensorRT-YOLO/demo/detect/</code> 下的 models 文件夹。同时工作目录现在默认也是 <code>/TensorRT-YOLO/demo/detect/</code> 。</p><p>然后，使用以下指令将模型导出为带有 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL05WSURJQS9UZW5zb3JSVC90cmVlL21haW4vcGx1Z2luL2VmZmljaWVudE5NU1BsdWdpbg==">EfficientNMS</span> 插件的 ONNX 格式：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>trtyolo <span class="token builtin class-name">export</span> <span class="token parameter variable">-w</span> yolov8s.pt <span class="token parameter variable">-v</span> yolov8 <span class="token parameter variable">-o</span> models</pre></td></tr></table></figure><p>执行以上命令后，将在 <code>models</code> 文件夹下生成名为 <code>yolov8s.onnx</code> 的文件。然后，使用 <code>trtexec</code> 工具将 ONNX 文件转换为 TensorRT engine：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>trtexec <span class="token parameter variable">--onnx</span><span class="token operator">=</span>models/yolov8s.onnx <span class="token parameter variable">--saveEngine</span><span class="token operator">=</span>models/yolov8s.engine <span class="token parameter variable">--fp16</span></pre></td></tr></table></figure><p>到这里都没哟遇到什么错误，第二个命令运行时我这里看起来像卡住了一样，不过最终是跑通了的。</p><h3 id="模型推理"><a class="anchor" href="#模型推理">#</a> 模型推理</h3><p>这里只讲 C++ 部分。</p><p>使用 C++ 进行推理前，请确保您已按照 <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2xhdWdoMTIzMjEvVGVuc29yUlQtWU9MTy9ibG9iL21haW4vZG9jcy9jbi9idWlsZF9hbmRfaW5zdGFsbC5tZCNkZXBsb3ktJUU3JUJDJTk2JUU4JUFGJTkx">Deploy 编译指南</span> 对 Deploy 进行了编译。</p><p>接着，使用 xmake 将 <code>detect.cpp</code> 编译为可执行文件：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>xmake f <span class="token parameter variable">-P</span> <span class="token builtin class-name">.</span> <span class="token parameter variable">--tensorrt</span><span class="token operator">=</span><span class="token string">"/path/to/your/TensorRT"</span> <span class="token parameter variable">--deploy</span><span class="token operator">=</span>/path/to/your/TensorRT-YOLO</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>xmake <span class="token parameter variable">-P</span> <span class="token builtin class-name">.</span> <span class="token parameter variable">-r</span></pre></td></tr></table></figure><p>注：上面是原文，这里的 <code>&quot;/path/to/your/TensorRT&quot;</code> 就是前面安装的 <span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIubnZpZGlhLmNvbS9kb3dubG9hZHMvY29tcHV0ZS9tYWNoaW5lLWxlYXJuaW5nL3RlbnNvcnJ0LzEwLjIuMC90YXJzL1RlbnNvclJULTEwLjIuMC4xOS5MaW51eC54ODZfNjQtZ251LmN1ZGEtMTIuNS50YXIuZ3o=">TensorRT 10.2.0.19 for CUDA 12.5, Linux x86_64</span> 下的一个路径， <code>your_path/TensorRT-10.2.0.19/targets/x86_64-linux-gnu/</code> 。后面的 deploy 路径就是你的 TensorRT-YOLO 路径。</p><p>在执行上述命令后，将在根目录的 <code>build</code> 目录下生成名为 <code>detect</code> 的可执行文件。最后，您可以直接运行可执行文件或使用 <code>xmake run</code> 命令进行推理。使用 <code>--help</code> 查看详细指令选项：</p><p>注；上面是原文，我这里生成的 <code>detect</code> 的可执行文件位于 <code>/TensorRT-YOLO/demo/detect/build/linux/x86_64/release/</code> 下，同时还有一个 <code>libdeploy.so</code> 文件也在这个目录下。</p><blockquote><p>要加速推理过程，请使用 <code>--cudaGraph</code> 指令，但此功能仅支持静态模型，不支持动态模型。</p></blockquote><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>xmake run <span class="token parameter variable">-P</span> <span class="token builtin class-name">.</span> detect <span class="token parameter variable">-e</span> models/yolov8s.engine <span class="token parameter variable">-i</span> images <span class="token parameter variable">-o</span> output <span class="token parameter variable">-l</span> labels.txt <span class="token parameter variable">--cudaGraph</span></pre></td></tr></table></figure><p>注：这里命令中的路径一定要正确，工作目录在 <code>/TensorRT-YOLO/demo/detect/</code> 下时，为：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>./build/linux/x86_64/release/detect <span class="token parameter variable">-e</span> models/yolov8s.engine <span class="token parameter variable">-i</span> images <span class="token parameter variable">-o</span> output <span class="token parameter variable">-l</span> labels.txt <span class="token parameter variable">--cudaGraph</span></pre></td></tr></table></figure><p>使用 xmake run 的命令没去试，直接运行 detect 就行了。输出图像在 output 文件夹下，生成文本描述 <code>labels.txt</code> 。</p></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2024-07-20 13:10:54" itemprop="dateModified" datetime="2024-07-20T13:10:54+08:00">2024-07-20</time> </span><span id="笔记/ArchLinux 安装 CUDA 、TensorRT  和TensorRT-YOLO 记录/" class="item leancloud_visitors" data-flag-title="" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>陌上尘 <i class="ic i-at"><em>@</em></i>陌上尘归处</li><li class="link"><strong>本文链接：</strong> <a href="http://yuanmxc.site/%E7%AC%94%E8%AE%B0/ArchLinux%20%E5%AE%89%E8%A3%85%20CUDA%20%E3%80%81TensorRT%20%20%E5%92%8CTensorRT-YOLO%20%E8%AE%B0%E5%BD%95/">http://yuanmxc.site/笔记/ArchLinux 安装 CUDA 、TensorRT 和TensorRT-YOLO 记录/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/C++/C++%E5%87%BD%E6%95%B0/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;cdn.jsdelivr.net&#x2F;gh&#x2F;Yuanmxc&#x2F;Picture@latest&#x2F;BlogPictures&#x2F;Background&#x2F;p11.png" title="C++ 函数"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> C++</span><h3>C++ 函数</h3></a></div><div class="item right"></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#archlinux-%E5%AE%89%E8%A3%85-cuda-tensorrt-%E5%92%8Ctensorrt-yolo-%E8%AE%B0%E5%BD%95"><span class="toc-number">1.</span> <span class="toc-text">ArchLinux 安装 CUDA 、TensorRT 和 TensorRT-YOLO 记录</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85tensorrt-%E5%92%8C-cuda"><span class="toc-number">1.1.</span> <span class="toc-text">安装 TensorRT 和 CUDA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tensorrt_yolo"><span class="toc-number">1.2.</span> <span class="toc-text">tensorrt_yolo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#deploy-%E7%BC%96%E8%AF%91"><span class="toc-number">1.3.</span> <span class="toc-text">Deploy 编译</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85-tensorrt_yolo"><span class="toc-number">1.4.</span> <span class="toc-text">编译安装 tensorrt_yolo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B7%91%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.5.</span> <span class="toc-text">跑模型推理示例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%87%BA"><span class="toc-number">1.5.1.</span> <span class="toc-text">模型导出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86"><span class="toc-number">1.5.2.</span> <span class="toc-text">模型推理</span></a></li></ol></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="陌上尘" data-src="//cdn.jsdelivr.net/gh/Yuanmxc/Blog@latest/images/avatar1.jpg"><p class="name" itemprop="name">陌上尘</p><div class="description" itemprop="description">不乱于心，不困于情；不畏将来，不念过往。</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">17</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">6</span> <span class="name">分类</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL1l1YW5teGM=" title="https:&#x2F;&#x2F;github.com&#x2F;Yuanmxc"><i class="ic i-github"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>链环</a><ul class="submenu"><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a></li><li class="item"><a href="/webstack/" rel="section"><i class="ic i-star"></i>网址</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"></div><span><a href="/%E7%AC%94%E8%AE%B0/ArchLinux%20%E5%AE%89%E8%A3%85%20CUDA%20%E3%80%81TensorRT%20%20%E5%92%8CTensorRT-YOLO%20%E8%AE%B0%E5%BD%95/" title="未命名">未命名</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ArchLinux/" title="分类于 ArchLinux">ArchLinux</a></div><span><a href="/Archlinux/ArchLinux%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/" title="ArchLinux使用记录">ArchLinux使用记录</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/C/" title="分类于 C++">C++</a></div><span><a href="/C++/C++%E5%87%BD%E6%95%B0/" title="C++ 函数">C++ 函数</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/note/" title="分类于 笔记">笔记</a></div><span><a href="/%E7%AC%94%E8%AE%B0/Linux_coding/" title="Linux_coding">Linux_coding</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/note/" title="分类于 笔记">笔记</a></div><span><a href="/%E7%AC%94%E8%AE%B0/Git%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/" title="Git使用笔记">Git使用笔记</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/%E6%9C%AA%E5%88%86%E7%B1%BB/Hexo+Github%20Pages%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" title="Hexo+Github Pages 搭建个人博客">Hexo+Github Pages 搭建个人博客</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/note/" title="分类于 笔记">笔记</a></div><span><a href="/%E7%AC%94%E8%AE%B0/CMake%E5%AD%A6%E4%B9%A0/" title="CMake学习">CMake学习</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/note/" title="分类于 笔记">笔记</a></div><span><a href="/%E7%AC%94%E8%AE%B0/%E7%9F%A5%E8%AF%86%E5%B0%8F%E7%82%B9/" title="知识小点">知识小点</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/%E6%9C%AA%E5%88%86%E7%B1%BB/TCP%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93/" title="TCP的可靠数据传输">TCP的可靠数据传输</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/note/" title="分类于 笔记">笔记</a></div><span><a href="/%E7%AC%94%E8%AE%B0/6.s081%E7%AC%94%E8%AE%B0/" title="6.s081笔记">6.s081笔记</a></span></li></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">陌上尘 @ 陌上尘归处</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">63k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">57 分钟</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"笔记/ArchLinux 安装 CUDA 、TensorRT  和TensorRT-YOLO 记录/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="//cdn.jsdelivr.net/gh/Yuanmxc/Blog@latest/js/app.js?v=0.2.5"></script></body></html>